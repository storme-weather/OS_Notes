Chapter 3 (Processes)
- Threads:
	- Introduction to Threads:
		- Important to understand processes and how processes and threads relate to each other
		- OS keeps track of processes through a process table, which has entries to store CPU register values, memory maps, open files, privileges, ect
		- Process is an active entity and is known as a program in execution
		- OSs take great care to ensure a process cannot accidently or maliciously affect other processes. Multiple processes can concurrently share the same CPU, and other resources are made 
		  transparent. The OS requires the hardware support to enforce this separation
		- Concurrency transparency comes at a high price. Think of when a new process is created. OS must make a complete independent address space. This means context switching between independent
		  address spaces is expendsive. 
		- A thread executes its own piece of code, independent from other threads (like processes). However, threads do not need a high degree of concurrency transparency. A thread system maintains
		  only the minimum amount of info to have a CPU be shared amoung several threads. This means a context switch for a thread is significantly cheaper than a context switch for a process
		- Also it is up to the user to keep threads belonging to the same process to keep to themselves, while processes have to keep to themselves
		- Multithreading leads to a performance gain
		- Thread packages are implemented in user space
		- Most important benefit in a single-threaded process is one thread can block the entire process
		- Threads allow the user to exploit parallelism when a program is executed on a multiprocessor system.
		- Processes require IPC to communicate with each other (shared memory or message passing). Threads share the same space, so communication is easy and cheaper for threads.
		- Lightweight process (LWP): runs in the context of a single heavyweight process, and there can be several LWPs per process.
		- LWPs work like this: when a LWP is made, it gets its own stack and is executes the scheduling routine in search of a thread to execute. If several existrrs, they each execute the scheduler.
		  The thread table is shared by the LWPs. Synchronization between LWPs does not require kernal support (need mutexes and semaphores)
		- Advantages of LWPs:
			-creating, destorying, and synchronizing threads is cheap and doesn't involve the kernal.
			- if a process has enough LWPs, a blacking system call will not suspend the entire process
			- application doens't need to know about the LWPs
			- can easily be used in multiprocessing environments by executing different LWPs on different CPUs
		- Drawback of LWPs:
			- still need to create and destrot LWPs which is just as expendsive as kernal threads, but only done occasionally
	- Threads in distributed systems:
		- provide a convenient means of allowing blocking system calls without blocking the entire process. That means this can be used in DS so it easier to communicate with multiple connections
		- Multithreaded clients:
			- to establish a high degree of distribution transparency, DS that operate in WANs may have to conceal long interprocess message times. The round trip delay can be hundereds of ms
			- Best way to hide the latency is to start the communicate and do something else immediately
				Ex) Consider what happens in web browsers. Have to fetch each element of a web document (HTML, images, text, ect), browser has to setup a TCP/IP connection, read incoming data,
				    and pass it to a reading component. Setting up the connection and reading the data are blocking operations. A web browser starts with fetching the HTML and displays it. 
				    To hide communication latencies, some browers start displaying data while it is still coming in. While the user sees the text, the browser is still working to get more data
				    Each thread sets up a separate connection to the server and pulls data. Setting up a connection and reading data from the server can be programmed using standard blocking
				    system calls. User should only notice delays in the display of images and such, but can still browse through the web document
			- Another benefit to using multithreaded web browsers, where several connections can be opened simultaneously. This is important because web servers can be replicated across multiple
			  machines meaning tons of connections can be served without a single server being overloaded
		- Multithreaded servers:
			- There are benefits for multithreaded clients, but most of the benefits exists for servers.
			- Benefits:
				- multithreading simplies server code a substantial amount
				- easier to develop servers that exploit parallelism to attain high performance
			- to understand the benefits of threads for server code, consider the organization of a file server that occasionally blocks waiting for the disk. The file server normally waits for an 
			  incoming request for a file operation, carries out the request, and sends back the reply. A popular organization has one thread being the dispatcher, which reads for incoming file 
			  operations. The requests are sent by clients to an end-point, and are placed into a worker thread. Worker proceeds by performing a blocking read on the local file system, which can
			  cause the thread to suspend until the data is fetched. If the thread is suspended, then another thread is selected to be executed. 
			- Running our example on a single threaded machine would result in less requests being served
			- There is one more way and that is through a finite state machine. Only one thread exists to respond to a request. If the request can be served through a cache, then good. Otherwise,
			  go to disk. However, the thread doesn't block and records the state of the request into a table, goes onto the next message.
- Virtualization:
	- resource virtualization: only having a single resource, but can pretend there are more
	- Role of virtualization in DS:
		- Virtualization deals with extending or replacing an existing interface to mimic the behavior of another system
		- Virtualization was introduced in the 70's to run legacy software on expendsive mainframe hardware.
		- Virtualization is easy due to software at high levels of abstraction are stable
		- Since networking has become pervasive, that makes it easy for clients to connect to VMs with various different types of OS and applications
		- Virtualization provides a high degree of portability and flexibility. Management of edge servers become much easier with virtualization. The reason is it can easily be replicated.
	- Architecture of VMs
		- 4 types of interface:
			1) interface between hardware and software consisting of machine instructions that can be invoked by any program
			2) interface between hardware and software consisting of machine instructions that can be invoked by privileged programs (OS)
			3) interface consisting of system calls as offered by an OS
			4) interface consisting of library calls, in the form of an API. System calls are hidden by the API
		- Virtualization takes place in 2 different ways
			1) can build a runtime system that provides and abstract instruction set that is used for executing applications. Instructions can be interpreted, but could also be 
			   emulated (think of WINE). This is known as a process virtual machine, stressing that virtualization is done only for a single process
			2) provide a system that is implemented as a layer shielding the original hardware, but offering an instruction set of that same or other hardware as an interface. The interface can be 
			   provided simultaneously to different programs. Because of this, it is possible to run multiple OS on the same machine. The layer is referred to as a virtual machine monitor (VMM). 
			   Think of VMware as an example
		- VMMs will become more important because of reliability and security for DS because they allow isolation of an application and its enviornment. A failure caused by an error or security attack
		  only affect the VMM. We can remove it and fire up a new one. Also VMMs can be moved from one machine to another easily.
		- binary translation: VMMs will translate instructions to that of the underlying machine while executing an application
		- sensitve instructions: when VMMs trap to the original kernal (system calls or privileged instructions)
- Clients:
	- Networked user interfaces:
		- major task of a client is to provide the means for users to interact with remote servers. 2 ways this can be supported:
			1) each remote service the client will have a counterpart that can contact the service over the network. An example would be an agenda running on a user's PDA that needs to sync with a
			   remote shared agenda
			2) provide direct access to remote services by offering a convenient user interface. The client is used only as a terminal with no need for local storage leading to an 
			   application-neutral solution. With networked user interfaces, everyything is processed and stored on the server. This thin-client approach is getting more attention as internet 
			   connectivity increases and hand-held become more popular. Thin-client solutions are popular as they ease the task of system management
		- X Window System:
			- This system is sued to control bit-mapped terminals, which include a monitor, keyboard, and mouse. X can be viewed as part of an OS that controls the terminal. Heart of the system is
			  formed by the X kernal. Contains all terminal-specific device drivers and is generally highly hardware dependent.
			- X kernal gives a low-level interface for controlling the screen, but also for capturing keyboard and mouse events. 
			- Good thing about X is the X kernal and applications don't need to be on the same machine. 
		- Thin-client network computing
			- In the X Windows system, applications manipulate a display using specific commands offered by X. These commands are sent over the network where they are executed byt he X kernal.
			  Applications written for X should use separate application logic from user-interface commands, but that is not always the case. Most of the time, the application logic and the user
			  interaction are tightly coupled. Basically, many requests are sent to the X kernal and a response is expected before making the next step. This is bad on WANs.
			- There are some solutions to the above problem. One of them is to re-engineer the implementation of the X protocol. Important part of the work concentrates on bandwidth reduction by
			  compressing messages. Messages are considered to be a fixed part, which is treated as an identifier, and a variable part. Multiple messages will have teh same identifier and often
			  contain the same data. This property can be used to send only the differences between messages having the same identifier. The sending and receiving sides maintain a local cache 
			  where the entries can be looked up using the identifier of the message. When a message is sent, look in the cache. If the
			  message is found, then a previous message with the same identifier, but with different data was sent. If that is the case, then differential encoding is sued to only send the 
			  differences between the 2 messages. At the receiving end, the message is also looked up in the cache, after decoding through the differences that can take plae. If there is a cache
			  miss, standard compression techniques are used that leads to factor 4 improvement in bandwidth. This technique has reported bandwidth reductions of up to 1000
			- Another solution is to keep the software at the display very simple and to let processing happen on the application side. This requires compression techniques in order to prevent
			  bandwidth availability to be a problem.
	- Client-side software for distribution transparency:
		- there is more to clients than user interfaces. Parts of the processing and data level in the client-server application are executed on the client side.
		- access transparency: client-side stubs for RPCs provides the same interface as the server, but hides the differences in machine architectures, as well as the actual communication
		- location/migration transparency: server lets the client-side software know when the server changes location, so the client can hide it from the user. The client rebounds to the server
		  location. At worst the client notices a temporary lost of performance
		- replication transparency: client stub send multiple requests to replicated servers and collect the incoming responses as one to the client
		- failure transparency: client middleware can be configured to repeatedly attempt to connect to a server or try another server after several failures. Sometimes the client middleware will
		  returned cached data
		- concurrency transparency: handled through special intermediate servers, transaction monitors, and requires less support from client software. Persistence transparenct is often completely
		  handled by the server
- Servers:
	- General Design Issues
		- Server is a process implementing a service on the behalf of clients.
		- A server does the following:
			1) Waits for an incoming request from a client
			2) takes care of the request
			3) Waits for another request
		- Iterative server: server handles the request, and returns a response to the requesting client.
		- Concurrent server: does not handle the client request itself, but passes it to a separate thread, and will wait for another request
		- A multithreaded server is an example of a concurrent server
		- Issue is where clients contact a server. Clients send requests to an end point (port) on the server. Each server will listen on a specific port. However, how do clients know whih port to
		  reach? Globally assign end points for well known services. Ex) servers that handle FTP requests listen to port 21. 
		- Some services do not require a preassigned end point. Sometimes the client needs to loop up the end point. One solution is to use a special daemon running on each machine on the server side.
		  The daemon itself listens to a well known end point. A client will contact the daemon, request the end point, then contact the specific server
			- Special Daemon: keeps track of the current end point of each service implemented by a server.
		- An issue with a daemon is having to implement each service as a separate server is a waste of resources.
		- Instead of using a daemon due to the issue (separate server for each service), we can use a single server known as a superserver.
			- Superserver: a single server listenting to each end point that is linked to a service
		- Another issue when designing a server is how can the server be interrupted? Example: say a user FTPs a file to the server, but it is the wrong file. A couple solutions can be used:
			- have the user exit the client application, which breaks the connection. This is basically a clean restart
			- make it possible to send out-of-band data which is data that is processed by the server before any other data from the client. One way to develop this is let the server listen to a
			  separate control end point where the client sends out-of-band data, while listening to the other end point containing the other data (will have lower priority). Another way to
			  implement this is to send out-of-band data across the same connection that has the original request.
		- Stateless server: does not keep info on the state of its clients, and doesn't have to inform clients on a state change. 
			Ex) a web server is stateless. It only responds to incoming HTTP requests which is for uploading a file to a server or fetching a file. When the request is done, then the server
			    forgets about the client. Also files on that server can be changed without notifying the clients connected to the server
		- Many stateless designs do contain some info on the clients, but if the server goes down for w/e reason, then the info is lost. However, the info wasn't overly important so nothing is done
		  to recover the info.
		- some stateless designs maintain a soft state. A soft state is where the server promises to maintain state for the client, but only for a limited amount of time. The server will fall back to
		  normal behavior after that time alloted has passed
		- Stateful server: maintains persistent info on the clients. Info needs to be deleted by the server, and doesn't automatically go away like soft state stateless machines
			Ex) file server that allows a client to keep a local copy of a file. Would need to update the file and its permissions accordingly when it is changed on the server
		- The example above shows how there are advantages to a stateful server (we know what users have the file). However, a drawback to this is if the server crashes, then all of that info has to
		  be recovered
		- With stateful servers, there are differences between temporary session and permanent states. Session states are often maintained in 3-tier client-server architectures, where the application
		  server needs accesss to a db server through a series of queries before responding to a client. No real harm is done if the session state is lost if the client can re-issue the original
		  request. Permanent state is for info maintained in dbs such as customer info, keys associated with purchased software, ect. 
		- Servers can also send a cookie to keep track of clients, but privacy is violated with this method
		- a stateless or stateful design should not affect the services provided by the server.
	- Server Clusters:
		- General Organization:
			- A server cluster is a collection of machines connected through a network, where each machine runs one or more servers. They are typically connected through a LAN
			- Typically they are organized in 3 tiers:
				1) first tier has a logical switch where client requests are routed. These switches can be transport-layer switches that accept incoming TCP connections and pass requests to
				   one of the servers in the cluster
				2) Second tier has application/compute servers that are high-performance hardware that deliver computing power. Could run on low end machines since compute power is not the 
				   bottleneck
				3) Third tier has data-processing servers (file and db servers). Depending on the usage of the cluster, these servers may run on specialized machines configured for high-speed
				   disk access, and have large server-side data caches
			- Server clusters won't always follow this organization. Sometimes each machine has its own local storage, integrating application and data processing in a single server, thus having a
			  2-tiered architecture
			- if a cluster offers multiple services, they can run on different application servers. The switch will not have to distinguish services or it cannot forward the requests to the proper
			  server.
			- certain machines can be temporarily idle, while others are overloaded. Can use virtual machines to migrate code to real machines
			- Important design goal for server clusters is to hide that there are multiple servers. Clients running on remote servers have no need to know about the organization of the cluster. 
			  This access transparency is done througha  single access point which is implemented through a hardware switch. Switch forms the entry point for the cluster, offering one network 
			  address. A cluster can have multiple access points where each access point has its own dedicated machine. This is done for scalability and availability.
			- To access a cluster, setup a TCP connection over, which application-level requests are sent as part of a session. Session ends by ending the connection. For transport-layer switches,
			  switch accepts incoming TCP connections and hands such connections to one of the servers. Known as a TCP handoff.
			- Switch plays an important role in distributing the load amongst various servers. They decide where the requests are sent, they also decide what server handles further processing of
			  the request. Simplest load-blanacing policy for a switch is round robin.
		- Distributed Servers:
			- Clusters discussed so far are statically configured. There is often a separate administration machine that keeps track of available servers, and pass this info to the switch
			- Most clusters have a single access point. When it fails, the cluster is unavailable. To avoid this, several access points can be provided.
				Ex) DNS can return several addresses, all belonging to the same host. This still requires clients to make several attempts if one of the access points fail, but at least it
				    isn't a hard failure
			- Desirable to have stability (long-living access point), but also want a high degree of flexibility in configuring the cluster. This leads to a design where there are a dynamically
			  changing set of machines with various access points, but looks like a single machine to clients. 
			- Basic idea of a distributed server is clients benefit from a robust, high-performing, stable server. Grouping simpler machines transparently into a cluster, and not relying on the
			  availability of one, it is possible to achieve a better degree of stability.
	- Managing server clusters:
		- Common aproaches:
			- most common approach with a cluster is to extend traditional managing functions of a single pc to a cluster. This means an administrator can into a node from a remote client to
			  execute commands to monitor, install, and change components
			- more advances is to hide that you need to log into a node, and provide an interface for an admin machine that collects info from one or more server, upgrade components, add/remove
			  nodes, ect. Main advantave of this approach is that collective operations operate on a group of servers. Known as Cluster Systems Management. Once the cluster gets huge though, can't
			  use this form of management.
			- Most of the support for super large clusters is ad hoc. No systematic approach for managing huge clusters.
		- PlanetLab Example
			- strange cluster server where different organizations each donate one of more computers adding up to hundered of nodes. These PCs form a 1-tier cluster where access, processing, and 
			  storage take place on each node individually.
